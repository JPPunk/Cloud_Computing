{"cells": [{"metadata": {"trusted": true}, "cell_type": "code", "source": "from pyspark.sql import SparkSession", "execution_count": 1, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "e34a3b138c754c7a8e28198fe197654b"}}, "metadata": {}}, {"output_type": "stream", "text": "Starting Spark application\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1558448443060_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-24-195.ec2.internal:20888/proxy/application_1558448443060_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-22-190.ec2.internal:8042/node/containerlogs/container_1558448443060_0002_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "text": "SparkSession available as 'spark'.\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "spark = SparkSession \\\n    .builder \\\n    .appName(\"COMP5349_ASS2\") \\\n    .getOrCreate()", "execution_count": 2, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "9ff782c84f7543568c23463afa29f047"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Stage 1"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "#load data\nmusic_data = 's3://amazon-reviews-pds/tsv/amazon_reviews_us_Music_v1_00.tsv.gz'\nmus = spark.read.csv(music_data, header=True,sep='\\t').cache()", "execution_count": 3, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "fe6a7f09e1a14e5d92a01fb28e3c7774"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Total number of reviews\nmus.count()", "execution_count": 4, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "b631dfddbcd445d09f2be2a70c325540"}}, "metadata": {}}, {"output_type": "stream", "text": "4751577", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# The number of unique users\nmus.dropDuplicates([\"customer_id\"]).count()", "execution_count": 5, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "51f14604cf68423ca16627b0a6e4a326"}}, "metadata": {}}, {"output_type": "stream", "text": "1940732", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# The number of unique products\nmus.dropDuplicates([\"product_id\"]).count()", "execution_count": 6, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "a974be71a3ca47b79d61d40fcfd78e28"}}, "metadata": {}}, {"output_type": "stream", "text": "782326", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# The largest number of reviews published by a single user\nuser_rev = mus.groupBy(\"customer_id\").count().orderBy(\"count\", ascending=False).cache()\nuser_rev.select(\"count\").first()[0]", "execution_count": 7, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "34ff4bb031af48f7b1e6ee189ce18fbc"}}, "metadata": {}}, {"output_type": "stream", "text": "7168", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# The top10 users ranked by the number of reviews they publish\nuser_rev.select(\"customer_id\").take(10)", "execution_count": 8, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "cfc705855a6846ff817fea65aa54828b"}}, "metadata": {}}, {"output_type": "stream", "text": "[Row(customer_id='50736950'), Row(customer_id='38214553'), Row(customer_id='51184997'), Row(customer_id='18116317'), Row(customer_id='23267387'), Row(customer_id='50345651'), Row(customer_id='14539589'), Row(customer_id='15725862'), Row(customer_id='19380211'), Row(customer_id='20018062')]", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# The median number of reviews published by a user\nuser_rev.approxQuantile(\"count\", [0.5], 0)[0]", "execution_count": 9, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "35b0dad65a054c6482ad0dbe66e0ed12"}}, "metadata": {}}, {"output_type": "stream", "text": "1.0", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# The largest number of reviews written for a single product\nprod_rev = mus.groupBy(\"product_id\").count().orderBy(\"count\", ascending=False).cache()\nprod_rev.select(\"count\").first()[0]", "execution_count": 10, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "bb1bb2793dc644eca6687b09d901a84f"}}, "metadata": {}}, {"output_type": "stream", "text": "3936", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# The top10 products ranked by the number of reviews they have\nprod_rev.select(\"product_id\").take(10)", "execution_count": 11, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "87978d53e4f1452a9111a8e0e687e3b1"}}, "metadata": {}}, {"output_type": "stream", "text": "[Row(product_id='B00008OWZG'), Row(product_id='B0000AGWEC'), Row(product_id='B00MIA0KGY'), Row(product_id='B00NEJ7MMI'), Row(product_id='B000089RVX'), Row(product_id='B004EBT5CU'), Row(product_id='B0026P3G12'), Row(product_id='B00009PRZF'), Row(product_id='B00004XONN'), Row(product_id='B00006J6VG')]", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# The median number of reviews a product has\nprod_rev.approxQuantile(\"count\", [0.5], 0)[0]", "execution_count": 12, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "16e8094edefb4b7fb64f33838658d399"}}, "metadata": {}}, {"output_type": "stream", "text": "2.0", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Stage 2"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Remove reviews published by users with less than median number of reviews published\n\n#but we got median number of reviews per user is 1.0, we decided to remove anyway.\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import count\nwindow = Window \\\n    .partitionBy(\"customer_id\") \\\n    #.orderBy(\"ts\")\nmus_cus = mus.withColumn(\"n\", count(\"customer_id\") \\\n    .over(window)) \\\n    .filter(\"n > 1\") \\\n    .drop(\"n\").cache()\nmus_cus.count()", "execution_count": 13, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "983c68b8174e4045914c5f86d3586539"}}, "metadata": {}}, {"output_type": "stream", "text": "3408839", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Remove reviews from products with less than median number of reviews received\nwindow = Window \\\n    .partitionBy(\"product_id\") \\\n    #.orderBy(\"ts\")\nmus_pro = mus_cus.withColumn(\"n\", count(\"product_id\") \\\n    .over(window)) \\\n    .filter(\"n > 2\") \\\n    .drop(\"n\").cache()\n\nmus_pro.count()", "execution_count": 14, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "ac31225c44f7410497130278bae60163"}}, "metadata": {}}, {"output_type": "stream", "text": "2858386", "name": "stdout"}]}, {"metadata": {"scrolled": true, "trusted": true}, "cell_type": "code", "source": "# Using \"\u2026 . ! ?\" to split sentences\n\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import ArrayType, StringType\nimport re\n\ndef split_and_remove_empty (x):\n    x = str(x)\n    x_row = re.split(r\"[.!?\u2026]\", x)\n    x_row_r = [x for x in x_row if x != \"\" and x != \" \"]\n    return x_row_r\n\nsplit_re = udf(split_and_remove_empty, ArrayType(StringType(), containsNull=False))\n# mus_pro.select(split_re(mus_pro.review_body).alias('split_re'))\n\nmus_pro_split = mus_pro.withColumn(\"split_re\", split_re(mus_pro.review_body).alias('review_body')).cache()\nmus_pro_split.count()", "execution_count": 20, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "a4e6f4a9349945f7a18d49d2b152d75a"}}, "metadata": {}}, {"output_type": "stream", "text": "2858386", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Remove reviews with less than two sentences in the review body. \n\nfrom pyspark.sql.functions import col, size\n\nmus_processed = mus_pro_split.filter(size(col(\"split_re\")) > 2).cache()\nmus_processed.count()", "execution_count": 16, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "c8e14d4abe994c24ab94fa5d4c6557e6"}}, "metadata": {}}, {"output_type": "stream", "text": "2187835", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Add a column to record the median of sentences\nmus_pro_split_count = mus_processed.withColumn(\"split_count\", size(col(\"split_re\"))-1).cache()", "execution_count": 17, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "604bfd14ee2d48c184172dfcdf113a28"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Top 10 users ranked by median number of sentences in the reviews they have published\n\nfrom pyspark.sql import Window\nimport pyspark.sql.functions as F\n\n#grp_window = Window.partitionBy('grp')\nmagic_percentile = F.expr('percentile_approx(split_count, 0.5)')\n\n#df.withColumn('med_val', magic_percentile.over(grp_window))\n\nmg = mus_pro_split_count.groupBy(\"customer_id\").agg(magic_percentile.alias('med_val'))\n\nmg.orderBy(\"med_val\", ascending=False).show(10)", "execution_count": 18, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "972a38b520164743bfe28f9e04a86a99"}}, "metadata": {}}, {"output_type": "stream", "text": "+-----------+-------+\n|customer_id|med_val|\n+-----------+-------+\n|   25628286|    272|\n|   50595705|    222|\n|   23717536|    219|\n|   43879820|    196|\n|   17821650|    184|\n|   15585529|    181|\n|   37733322|    167|\n|   49916132|    158|\n|   22109829|    157|\n|   44348856|    157|\n|   27767670|    154|\n|   12365011|    152|\n|   36306364|    150|\n|   35279755|    146|\n|   14544989|    146|\n|   21065408|    144|\n|   19493393|    143|\n|   53024265|    140|\n|   29061899|    139|\n|   16273696|    139|\n+-----------+-------+\nonly showing top 20 rows", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Top 10 products ranked by median number of sentences in the reviews they have published\n\nfrom pyspark.sql import Window\nimport pyspark.sql.functions as F\n\n#grp_window = Window.partitionBy('grp')\nmagic_percentile = F.expr('percentile_approx(split_count, 0.5)')\n\n#df.withColumn('med_val', magic_percentile.over(grp_window))\n\nmg_p = mus_pro_split_count.groupBy(\"product_id\").agg(magic_percentile.alias('med_val'))\n\nmg_p.orderBy(\"med_val\", ascending=False).show(10)", "execution_count": 19, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "39c3268fb2224c3a8e1ef31ce1d71166"}}, "metadata": {}}, {"output_type": "stream", "text": "+----------+-------+\n|product_id|med_val|\n+----------+-------+\n|B000003G29|    466|\n|B005ZHBBU6|    320|\n|B008LA8E9K|    205|\n|B00IROI9BS|    187|\n|B00AP5M4WM|    175|\n|B000S5LQBO|    163|\n|B000ASAEI0|    163|\n|B007929F9E|    160|\n|B00LF0GLXO|    155|\n|B000KJTCOG|    153|\n|B0074B10ES|    151|\n|B00L47E4VE|    147|\n|B000002B6S|    144|\n|B00644JI1S|    137|\n|B00N3B2290|    133|\n|B00TILNZFY|    133|\n|B00061NSU2|    128|\n|B0000BHIKV|    127|\n|B00KFTW0ZE|    126|\n|B003Z4Y5HW|    125|\n+----------+-------+\nonly showing top 20 rows", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Stage 3"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Select 1 product from top 10 product\nmus_1_pro = mus_processed.filter(mus_processed['product_id']=='B00006J6VG').cache()", "execution_count": 20, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f463ad43f5a04a9b8c43d80fd88f694c"}}, "metadata": {}}, {"output_type": "stream", "text": "787", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Divide into positive and negative\nmus_1_pro_pos = mus_1_pro.filter(\"star_rating >= 4\").cache()\nmus_1_pro_neg = mus_1_pro.filter(\"star_rating <= 2\").cache()", "execution_count": 21, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "515e385db2a2441282e905f2074f25f4"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Make index RDD to find back the review id and review body \n\ndef flat_id(tup):\n    for val in tup[1]:\n        yield tup[0]\n\ndef flat_val(tup):\n    for val in tup[1]:\n        yield val\n\n### Negative\nmus_1_pro_neg_id_rdd = mus_1_pro_neg.rdd \\\n                    .map(lambda p: (p.review_id, p.split_re)) \\\n                    .flatMap(flat_id)\n\nmus_1_pro_neg_val_rdd = mus_1_pro_neg.rdd \\\n                    .map(lambda p: (p.review_id, p.split_re)) \\\n                    .flatMap(flat_val)\n\nid_list_neg = mus_1_pro_neg_id_rdd.collect()\nval_list_neg = mus_1_pro_neg_val_rdd.collect()\n\nneg_input = sc.parallelize(val_list_neg)\n\n### positive\nmus_1_pro_pos_id_rdd = mus_1_pro_pos.rdd \\\n                    .map(lambda p: (p.review_id, p.split_re)) \\\n                    .flatMap(flat_id)\n\nmus_1_pro_pos_val_rdd = mus_1_pro_pos.rdd \\\n                    .map(lambda p: (p.review_id, p.split_re)) \\\n                    .flatMap(flat_val)\n\nid_list_pos = mus_1_pro_pos_id_rdd.collect()\nval_list_pos = mus_1_pro_pos_val_rdd.collect()\n\npos_input = sc.parallelize(val_list_pos)", "execution_count": 22, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "3d8f4673d91f4ccbb1a97243ec68c614"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "### Sentence embedding\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\ndef review_embed(rev_text_partition):\n    module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]\n    embed = hub.Module(module_url)\n    # mapPartition would supply element inside a partition using generator stype\n    # this does not fit tensorflow stype\n    rev_text_list = [text for text in rev_text_partition]\n    with tf.Session() as session:\n        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n        message_embeddings = session.run(embed(rev_text_list))\n    return message_embeddings", "execution_count": 23, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "5c0dab94836746ac91fa5020577f33af"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Cache embedded data\nmus_1_pro_neg_embedding = neg_input.mapPartitions(review_embed)\n\nmus_1_pro_pos_embedding = pos_input.mapPartitions(review_embed)", "execution_count": 24, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "5352bac0e84342328ab9213b76a7efa4"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "#Using numpy to calculate cosine distance\n#Cosine distance = 1 - cosine similarity\n#Since google universal sentence encoder returns normalized value, \n#the inner product of encodings can be treated as a cosine similarity matrix\n#                       x * y\n#cosine similarity = -----------\n#                     |x| * |y|\n#|x| == |y| == 1\nimport numpy as np\n\nneg_embedding_list = mus_1_pro_neg_embedding.collect()\nneg_matrix = np.inner(neg_embedding_list, neg_embedding_list)\npos_embedding_list = mus_1_pro_pos_embedding.collect()\npos_matrix = np.inner(pos_embedding_list, pos_embedding_list)\n\n#Use identity matrix - similarity matrix to calculate distance matrix\ni_matrix_neg = np.ones(neg_matrix.shape)\nneg_matrix_distance = i_matrix_neg - neg_matrix\ni_matrix_pos = np.ones(pos_matrix.shape)\npos_matrix_distance = i_matrix_pos - pos_matrix\n\n#Calculate average distance of one sentence to all other sentences\nline_distance_neg = neg_matrix_distance.sum(axis=0)/len(neg_matrix_distance)\nline_distance_pos = pos_matrix_distance.sum(axis=0)/len(pos_matrix_distance)\n\n#Find out center sentence in each class\ncen_neg = np.argmin(line_distance_neg)\ncen_pos = np.argmin(line_distance_pos)\n\n#Find out 10 nearest sentences to center sentence in each class\ncen_ten_neg = np.argsort(neg_matrix_distance[cen_neg])\ncen_ten_pos = np.argsort(pos_matrix_distance[cen_pos])\n\n#Print output\n# print(\"negative class average distance:\")\n# print(neg_matrix_distance.mean())\n# print(\"positive class average distance:\")\n# print(pos_matrix_distance.mean())\n\n#Print output\nprint(\"negative center sentence:\")\nprint(id_list_neg[cen_neg],val_list_neg[cen_neg])\nprint(\"negative nearest 10 sentences:\")\nfor x in cen_ten_neg[1:11]:\n  print(id_list_neg[x],val_list_neg[x].strip())\n\nprint(\"positive center sentence:\")\nprint(id_list_neg[cen_neg],val_list_pos[cen_pos])\nprint(\"positive nearest 10 sentences:\")\nfor x in cen_ten_pos[1:11]:\n  print(id_list_pos[x],val_list_pos[x].strip())\n", "execution_count": 25, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "5127bbd69c2242009a14325a57c1cbea"}}, "metadata": {}}, {"output_type": "stream", "text": "negative center sentence:\nR2L4PZC7CHGQ4R  This album is much too insipid for people who like to listen to real music\nnegative nearest 10 sentences:\nR36BAXRVCR2PFB I bet most people who like this album don't even like real punk\nR2UBEJ5JX4PKX1 Almost everything that is wrong with mainstream music today, is represented in this album\nR195DYX83KWYXU Simply put, this album is mediocre at best\nR3QG1GKJO8IL4K This album has in no way let down that genre\nR1DP63VJ4NXY44 This album absolutely sucks\nR1DP63VJ4NXY44 This album is full of teeny-bopper pop songs disguised as rock\nR1CA8OIEZ0B22Y I do like Blink 182 but most of this type of this music is junk\nRKN17VFTZQ69P This album is horrid\nRATB9UCW9ZV0B This album is a waste of time and money\nRKEOBZVEWY7RW those sad people saying this is 'the greatest album in years' need to listen to some proper music, not watch Mtv all day\npositive center sentence:\nR2L4PZC7CHGQ4R  just an awesome song\npositive nearest 10 sentences:\nR3OSCPFNN3SAAS AWESOME SONG\nR2EKY5I5KJW2PB GREAT SONG\nR2H1YNHUCT31TS another good song\nRY3PCDQ80U0O8 Not really a \\\\\"song\\\\\", but it's awesome\nR3OSCPFNN3SAAS Cool song\nR2RZANPP1RFQ7M Wondering- Now this song is awesome\nR1BMO598TXFB7R very good song\nR1W2RU1V7J3IGI this is an awsome song\nR2RZANPP1RFQ7M its a really disapointing song, but really good\nRM2Z9V7TKMH7P Movin' On - AWESOME SONG", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Stage 4"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "##convert to df to process\nfrom pyspark.sql import Row\nrow = Row(\"val\") # Or some other column name\nneg_input_df = neg_input.map(row).toDF()\npos_input_df = pos_input.map(row).toDF()", "execution_count": 26, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "b2b40d40a57d44278c601b5f6d3379ca"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "##define a function remove punctuation and tab and split token \nimport re\nfrom pyspark.sql import Row\ndef normalize_name(name):\n    name = re.sub(r\"[.,\\/#!$%\\^\\*;:{}=\\_`~()@]\", ' ', name)\n    name = re.sub(r'\\s+', ' ', name).strip()\n    name = [tok for tok in re.split(\" \",name) if tok]\n    return name", "execution_count": 27, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "94d231b4a933487e931ff8dffc555014"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "##use udf to preprocessing the review.\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import ArrayType,StringType\nnormal = udf(normalize_name,ArrayType(StringType()))\nneg_input_df_token = neg_input_df.withColumn(\"vec\",normal(neg_input_df['val'])).drop(\"val\")\nneg_input_df_token.show()\npos_input_df_token = pos_input_df.withColumn(\"vec\",normal(pos_input_df['val'])).drop(\"val\")\npos_input_df_token.show()\n# from pyspark.ml.feature import StopWordsRemover\n# swr = StopWordsRemover(inputCol = 'vec', outputCol = 'data')\n\n# neg_input_df_token_swr = swr.transform(neg_input_df_token).drop(\"vec\")\n# neg_input_df_token_swr.show(3)", "execution_count": 28, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "de62804dfb1843ed922b8144f1fd7795"}}, "metadata": {}}, {"output_type": "stream", "text": "+--------------------+\n|                 vec|\n+--------------------+\n|               [yep]|\n|[I, guess, the, l...|\n|[the, happiest, d...|\n|[this, group, is,...|\n|[its, another, bo...|\n|[its, all, an, im...|\n|[I'm, not, sure, ...|\n|[oh, hell, they'r...|\n|[Stop, infesting,...|\n|[<br, >, <br, >Lo...|\n|[realise, there, ...|\n|[I, HATE, this, a...|\n|[Whoever, said, t...|\n|[The, songs, are,...|\n|[Don't, waste, yo...|\n|[<BR>I'd, have, g...|\n|[This, album, abs...|\n|[Whoever, said, t...|\n|[The, songs, on, ...|\n|[This, album, is,...|\n+--------------------+\nonly showing top 20 rows\n\n+--------------------+\n|                 vec|\n+--------------------+\n|[The, Young, And,...|\n|[A, New, Beginnin...|\n|[The, Anthem, -, ...|\n|[Lifestyles, of, ...|\n|[Wondering, -, 2,...|\n|[The, Story, of, ...|\n|[Girls, &, Boys, ...|\n|[My, Bloody, Vale...|\n|[Hold, On, -, 5, ...|\n|[Riot, Girl, -, 5...|\n|[Say, Anything, -...|\n|[The, Day, that, ...|\n|[The, Young, and,...|\n|[Emotionless, -, ...|\n|  [Movin, On, -4, 5]|\n|[I, really, like,...|\n|[At, first, I, wa...|\n|[It, was, hard, f...|\n|[However, after, ...|\n|[While, this, CD,...|\n+--------------------+\nonly showing top 20 rows", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "##add index to all review_split\n##convert df to rdd to tfidf and cos-sim easily\nneg_list_id = [i for i in range(len(id_list_neg))]\npos_list_id = [i for i in range(len(id_list_pos))]\n\nneg_data_array = [row['vec'] for row in neg_input_df_token.collect()]\npos_data_array = [row['vec'] for row in pos_input_df_token.collect()]\n\nneg_data_rdd = sc.parallelize(neg_data_array)\npos_data_rdd = sc.parallelize(pos_data_array)\n\nneg_ind_rdd = sc.parallelize(neg_list_id)\npos_ind_rdd = sc.parallelize(pos_list_id)\n##using HashingTF, hashing 100000 dimension vector, reflect all word on it.\nfrom pyspark.mllib.feature import HashingTF, IDF\nhashingTF = HashingTF(100000)\ntf_neg = hashingTF.transform(neg_data_rdd)\ntf_pos = hashingTF.transform(pos_data_rdd)\ntf_neg.cache()\ntf_pos.cache()\n###compute tfidf and convert all sentence to vecotor\nidf_neg = IDF().fit(tf_neg)\ntfidf_neg = idf_neg.transform(tf_neg)\nidf_pos = IDF().fit(tf_pos)\ntfidf_pos = idf_pos.transform(tf_pos)\n\n### all vector from tfidf normalized and compute cos similarity\nfrom pyspark.mllib.feature import Normalizer\nnormalizer = Normalizer()\ndata_neg = neg_ind_rdd.zip(normalizer.transform(tfidf_neg))\ndata_pos = pos_ind_rdd.zip(normalizer.transform(tfidf_pos))\n##using cartesian product get all comination with each vector point and compute dot with them separately.\nneg_data_cos = data_neg.cartesian(data_neg)\\\n                .map(lambda l: ((l[0][0], l[1][0]), l[0][1].dot(l[1][1])))\\\n                .cache()\n\npos_data_cos = data_pos.cartesian(data_pos)\\\n                .map(lambda l: ((l[0][0], l[1][0]), l[0][1].dot(l[1][1])))\\\n                .cache()\n##convert to df and rename column\nneg_data_cos_rdd = neg_data_cos.map(lambda j: (j[0][0], j[0][1], j[1])).map(lambda j : (int(j[0]),int(j[1]), float(j[2])))\ntest_neg_df = neg_data_cos_rdd.toDF()\nneg_cos_mar = test_neg_df.selectExpr(\"_1 as i\", \"_2 as j\" ,\"_3 as cos\")\n\n\npos_data_cos_rdd = pos_data_cos.map(lambda j: (j[0][0], j[0][1], j[1])).map(lambda j : (int(j[0]),int(j[1]), float(j[2])))\ntest_pos_df = pos_data_cos_rdd.toDF()\npos_cos_mar = test_pos_df.selectExpr(\"_1 as i\", \"_2 as j\" ,\"_3 as cos\")\n\n", "execution_count": 29, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "399395b6104c4d6ab36ae25297bee96f"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "###compute 1-cos and get the center sentence\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import DoubleType, IntegerType\ndef red_1(val):\n    return 1-val\n\nreduce_1 = udf(red_1, DoubleType())\nneg_cos_df_cos_1 = neg_cos_mar.withColumn(\"1-cos\", reduce_1(neg_cos_mar.cos).alias(\"cos\"))\\\n                .drop(\"cos\")\n\npos_cos_df_cos_1 = pos_cos_mar.withColumn(\"1-cos\", reduce_1(pos_cos_mar.cos).alias(\"cos\"))\\\n                .drop(\"cos\")\n\nneg_cos_df_cos_1.groupBy('i').sum('1-cos').orderBy(\"sum(1-cos)\").show(10)\npos_cos_df_cos_1.groupBy('i').sum('1-cos').orderBy(\"sum(1-cos)\").show(10)\nprint(\"negative reviews center \", id_list_neg[1954], val_list_neg[1954])\nprint(\"---------------\")\nprint(\"positive reviews center \", id_list_pos[4909], val_list_pos[4909])\nprint(\"-----------------------\")\n", "execution_count": 30, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "6835d33a1ff84b1d8853567360f23a05"}}, "metadata": {}}, {"output_type": "stream", "text": "+----+------------------+\n|   i|        sum(1-cos)|\n+----+------------------+\n|1954| 3003.172262305627|\n|2018|3006.2644375244627|\n|1361|3012.1878366297365|\n|2962|3013.6289608350694|\n|2726|3016.7240764834323|\n|1503| 3017.455062704513|\n|1084| 3019.195963646621|\n|3051| 3020.394893524665|\n|1545| 3022.601971291658|\n|1826|3023.2426404057132|\n+----+------------------+\nonly showing top 10 rows\n\n+----+-----------------+\n|   i|       sum(1-cos)|\n+----+-----------------+\n| 823|4909.972432796806|\n|3062|4922.339457731504|\n|3850|4928.213983743272|\n| 291|4931.992705301235|\n| 968|4933.544159004289|\n|4040|4937.001879685455|\n|4927|4937.547664683688|\n|4484|4939.725924378351|\n|4159|4941.830710425537|\n|3834|4941.884629920205|\n+----+-----------------+\nonly showing top 10 rows\n\nnegative reviews center  RV9TLI1TKAOYP  I play music and I am rating Good Charlotte on their music which is just plain horrible, I mean they have five people in their band, the lyrics are terrible but catchy, and if you are in high school this should be out of your CD player, if your in middle school then it is understandable\n---------------\npositive reviews center  R9C2ZW7ODCQRT but those are my personal opinions\n-----------------------", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "##show top 10 sentence arount the neg/pos center \nprint(\"negative reviews center \", id_list_neg[1954], val_list_neg[1954])\nprint(\"---------------\")\nneg_stage4_list = neg_cos_df_cos_1.filter(\"i==1954\").orderBy(\"1-cos\").select(\"j\").head(11)\nneg_stage4_list_10 = [int(row.j) for row in neg_stage4_list[1:11]]\nprint(\"top 10 sentence of negative center\")\nfor k in neg_stage4_list_10:\n    print(id_list_neg[k], val_list_neg[k])\nprint(\"---------------------\")    \n    \n    \nprint(\"positive reviews center \", id_list_pos[4909], val_list_pos[4909])\nprint(\"-----------------------\")\npos_stage4_list = pos_cos_df_cos_1.filter(\"i==4909\").orderBy(\"1-cos\").select(\"j\").head(11)\npos_stage4_list_10 = [int(row.j) for row in pos_stage4_list[1:11]]\nprint(\"top 10 sentence of postive center\")\nfor k in pos_stage4_list_10:\n    print(id_list_pos[k], val_list_pos[k])", "execution_count": 31, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "69057d755aa4486098bd77ecfeffac35"}}, "metadata": {}}, {"output_type": "stream", "text": "negative reviews center  RV9TLI1TKAOYP  I play music and I am rating Good Charlotte on their music which is just plain horrible, I mean they have five people in their band, the lyrics are terrible but catchy, and if you are in high school this should be out of your CD player, if your in middle school then it is understandable\n---------------\ntop 10 sentence of negative center\nR2VP9WX4XP2YSB   Just go out and buy their new cd and be happy that you can now \\\\\"fit in\\\\\" with the punk rock kiddies in your high school\nR317FWCY8Q6I4V   <br /> <br />Also, when you look at how unfortunate so many people in this country and this world are, you might see that you really don't have it all that bad in high school\nR1P6AETM3HJIDO Good Charlotte used to be a good band but they sold out and now they are horrible\nR3SW6OMLQW51ZJ  Their music is much more pop-like, their songs aren't about how they got picked on in high school and hating the &quot;in-crowd&quot; anymore\nRV9TLI1TKAOYP  They have made the average high school idiot think that being punk is about your clothes and how many peircings you have, and again I can understand why punks want to go Columbine on the preppy people who think Good Charlotte is rated in the same class as Anti Flag or The Misfits\nR317FWCY8Q6I4V  You will meet a lot of crappy people, yes, but they are just living the same confused high school years as you are\nR2XYJPVZMVCAVA This is a terrible CD\nRJETSQ5849YZJ I am so sick of these fat chicks at school who love Good Charlotte\nR317FWCY8Q6I4V  It's not like being part of the \\\\\"different\\\\\" crowd in high school destroyed their lives\nR24DPPKVHC5Q7G     If people want to say that Good Charlotte is good, which they are not, then go ahead\n---------------------\npositive reviews center  R9C2ZW7ODCQRT but those are my personal opinions\n-----------------------\ntop 10 sentence of postive center\nR6FC8EPBO348M I'm going to go over all the songs and my opinions on them:<BR>(1\nRJI7C4R8KYUND &quot; Good Charlotte is one of those bands\nRCOQKNY4D2S  I'm sorry to say that to those people who complain that GC is not punk, and to those people who believe that if you watch MTV, you are not a true fan, you are (in my opinion) very wrong\nR2SZ6V79UNC06Y  Man those  twins are strange\nR11IMFOZE8E6FO This is a much more personal album than their debut\nRKMQGZZSPMMHN <br />To all their fans GC will never be considered 'sell-outs' or 'fakes' and that is what matters regardless of others' opinions\nRROM5LBE67KNY   My personal favorite is the song titled The Young and the Hopeless\nR3GFLI255PW5MZ   My personal favorite is the song titled The Young and the Hopeless\nR1A3FKJY2K6UG0  Good Charlotte are one of those rare REAL bands that are on TRL\nRRF21SVDXP1YE   <br /> <br />My personal favorites are Young and Hopeless, Anthem, and Emotionless", "name": "stdout"}]}], "metadata": {"kernelspec": {"name": "pysparkkernel", "display_name": "PySpark", "language": ""}, "language_info": {"name": "pyspark", "mimetype": "text/x-python", "codemirror_mode": {"name": "python", "version": 2}, "pygments_lexer": "python2"}}, "nbformat": 4, "nbformat_minor": 2}
